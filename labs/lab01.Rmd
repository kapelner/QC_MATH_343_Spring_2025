---
title: "Practice Assignment 1 MATH 343"
author: "Your Name Here"
output: pdf_document
date: "noon February 28"
---

This practice assignment is coupled to the theory assignment (the problem numbers align herein) and should be worked on concomitantly.

You should have R and RStudio (latest versions) installed to edit this file. You will write code in places marked "TO-DO" to complete the problems. Most of this will be a pure programming assignment but there are some questions that instead ask you to "write a few sentences" which are not R chunks.

The tools for solving these problems can be found in the class demos located [here](https://github.com/kapelner/QC_MATH_343_Spring_2024/tree/main/demos). I prefer you to follow the methods from these examples. If you google and find esoteric code you don't understand or if you use chat GPT, this doesn't do you too much good in the long run.

To "hand in" the homework, you should follow the github repo setup instructions on the course homepage. Once you have your own class repo e.g. located in ~, make a /labs directory. Then go back to ~ and clone the class repo. Then copy this file into your repo/labs directory. Edits made there can be committed and pushed. You must push this completed file by the due date to avoid late penalties.

This lab requires the following packages. You should make sure they load before beginning:

```{r}
pacman::p_load(ggplot2, survival, optimx)
```

## Problem 1: Gibbs Sampler

Problem 3 in the theory homework assumes a normal change point model. We will make up some data and visualize it using a scatterplot:

```{r}
set.seed(1)
n = 50

true_theta_1 = 3
true_theta_2 = 6
true_sigsq_1 = 1.5
true_sigsq_2 = 0.5
true_theta_3 = 37

x = c(rnorm(true_theta_3, true_theta_1, true_sigsq_1), rnorm(n - true_theta_3, true_theta_2, true_sigsq_2))

ggplot(data.frame(t = 1 : n, x = x)) + 
  geom_point(aes(x = t, y = x))
```

Imagine you are looking at this dataset of counts for the first time (i.e. forget that we generated the data ourselves). Why do you think athe model is appropriate?

#TO-DO

We will now implement the Gibbs sampler given what you derived in the theoretical assignment. You found that I've provided some boilerplate code from the class demos below. It is your job to do the sampling.

```{r}
num_tot_samples = 1e4
theta_3s = array(NA, num_tot_samples)
theta_1s = array(NA, num_tot_samples)
sigsq_1s = array(NA, num_tot_samples)
theta_2s = array(NA, num_tot_samples)
sigsq_2s = array(NA, num_tot_samples)

###initialize thetas to be null values
theta_3s[1] = #TO-DO
theta_2s[1] = .Machine$double.eps
theta_3s[1] = .Machine$double.eps

###do the sampling
for (s in 2 : num_tot_samples){
  
}
```
Now we aggregate all three chains together for convenience:

```{r}
gibbs_chain = data.frame(
  theta_3 = theta_3s,
  theta_1 = theta1s, 
  sigsq_1 = sigsq_1s,
  theta_2 = theta2s, 
  sigsq_2 = sigsq_2s,
  t = 1 : num_tot_samples
)
rm(theta1s, theta2s, theta3s, sigsq_1s, sigsq_2s)
```


We now assess convergence using plots. Feel free to play around with the `max_t_for_plotting` to get a better visual on the beginning of the chains.

```{r}
max_t_for_plotting = 500
ggplot(gibbs_chain) +
  geom_point(aes(x = t, y = theta_1)) + 
  xlim(0, max_t_for_plotting)
ggplot(gibbs_chain) +
  geom_point(aes(x = t, y = theta_2)) + 
  xlim(0, max_t_for_plotting)
ggplot(gibbs_chain) +
  geom_point(aes(x = t, y = theta_3)) + 
  xlim(0, max_t_for_plotting)
ggplot(gibbs_chain) +
  geom_point(aes(x = t, y = sigsq_1)) + 
  xlim(0, max_t_for_plotting)
ggplot(gibbs_chain) +
  geom_point(aes(x = t, y = sigsq_2)) + 
  xlim(0, max_t_for_plotting)
#cleanup
rm(max_t_for_plotting)
```

Where do we burn?

```{r}
t_burn_in = #TO-DO
```

Now we burn:

```{r}
gibbs_chain = #TO-DO
```

Now we assess autocorrelation. Play with the `ell_max` and `r_max` to get the best assessment possible:

```{r}
par(mfrow = c(3, 1))
ell_max = 500
r_max = 1
acf(gibbs_chain$theta_1, 
    xlim = c(0, ell_max + 10), ylim = c(0, r_max), lag.max = ell_max)
acf(gibbs_chain$theta_2, 
    xlim = c(0, ell_max + 10), ylim = c(0, r_max), lag.max = ell_max)
acf(gibbs_chain$theta_3, 
    xlim = c(0, ell_max + 10), ylim = c(0, r_max), lag.max = ell_max)
acf(gibbs_chain$sigsq_1, 
    xlim = c(0, ell_max + 10), ylim = c(0, r_max), lag.max = ell_max)
acf(gibbs_chain$sigsq_2, 
    xlim = c(0, ell_max + 10), ylim = c(0, r_max), lag.max = ell_max)
#cleanup
rm(ell_max, r_max)
```

Where do we thin?

```{r}
t_thin = #TO-DO
```

Now we thin:

```{r}
gibbs_chain = #TO-DO
```

How many iid samples do we have after burning and thinning?

```{r}
#TO-DO
```

What would we change in the above code so we could've had more iid samples?

#TO-DO

Before we do inference, we first source the convenience function we used in class:

```{r}
visualize_chain_and_compute_estimates_and_cr = function(
    samples, 
    plot_mmse = TRUE, 
    plot_mmae = TRUE, 
    true_value = NULL, 
    alpha = NULL, 
    bins = 30,
    colors = c("blue", "orange", "green", "red")){
  ggplot_obj = ggplot(data.frame(samples = samples)) +
    geom_histogram(aes(x = samples), bins = bins)
  
  mmse = mean(samples)
  mmae = median(samples)
  if (plot_mmse){
    ggplot_obj = ggplot_obj + geom_vline(xintercept = mmse, col = colors[1])
  }
  
  if (plot_mmae){
    ggplot_obj = ggplot_obj + geom_vline(xintercept = mmae, col = colors[2])
  }
  
  if (!is.null(true_value)){
    ggplot_obj = ggplot_obj + 
      geom_vline(xintercept = true_value, col = colors[3]) 
  }
  if (!is.null(alpha)){
    ggplot_obj = ggplot_obj + 
      geom_vline(xintercept = quantile(samples, .025), col = colors[4]) + 
      geom_vline(xintercept = quantile(samples, .975), col = colors[4])
  }
  plot(ggplot_obj)
  
  ret = list(
    mmse = mmse,
    mmae = mmae,
    theta = true_value
  )
  if (!is.null(alpha)){
    ret$cr_one_minus_alpha_theta = c(
        quantile(samples, alpha / 2), 
        quantile(samples, 1 - alpha / 2)
      )
  }
  ret
}

```

Now we do inference on all three parameters:

```{r}
visualize_chain_and_compute_estimates_and_cr(gibbs_chain$theta_3, true_value = true_theta_3, alpha = 0.05)
```

How accurate was our inference on this parameter (the change point)?

#TO-DO

```{r}
visualize_chain_and_compute_estimates_and_cr(gibbs_chain$theta_1, true_value = true_theta_1, alpha = 0.05)
```

How accurate was our inference on this parameter (the first mean)?

#TO-DO

```{r}
visualize_chain_and_compute_estimates_and_cr(gibbs_chain$theta_2, true_value = true_theta_2, alpha = 0.05)
```

How accurate was our inference on this parameter (the second mean)?

```{r}
visualize_chain_and_compute_estimates_and_cr(gibbs_chain$sigsq_1, true_value = true_sigsq_1, alpha = 0.05)
```

How accurate was our inference on this parameter (the first variance)?

```{r}
visualize_chain_and_compute_estimates_and_cr(gibbs_chain$sigsq_2, true_value = true_sigsq_2, alpha = 0.05)
```

How accurate was our inference on this parameter (the second variance)?

#TO-DO

Now we do inference manually from the chain itself.

Find the MMSE and the MMAE for theta_1

```{r}
#TO-DO
```

Find a 95\% CR for theta_2

```{r}
#TO-DO
```

Test H_a: theta_3 < 25.

```{r}
#TO-DO
```

Masters students only: test H_a: sigsq_1 < sigsq_2.

```{r}
#TO-DO
```


## Problem 2: The Permutation Test

We will analyze the anorexia dataset in MASS. You can read about it here:

```{r}
rm(list = ls())
?MASS::anorexia
D = MASS::anorexia
```

The data is measured on 72 people before and after treatment. We are interested in the outcome of percentage change in weight. So we first create that variable:

```{r}
D$y = (D$Postwt - D$Prewt) / D$Prewt
```

We will be interested in if there's a different between the two treatment groups: cognitive behavioral treatment (CBT) vs family treatment (FT). So we separate the two datasets now:

```{r}
x1 = D$y[D$Treat == "FT"]
x2 = D$y[D$Treat == "CBT"]
x = c(x1, x2)
n1 = length(x1)
n2 = length(x2)
rm(D)
```

How many possible ways are there to "permute" the dataset into two groups

```{r}
#TO-DO
```

Is it possible to run this many permutations? Yes / no

Instead let's run `B = 100,000`. Pick a test statistic and compute the test statistic for all  

```{r}

test_stat = function(samp1, samp2){
  #TO-DO
}

B = 1e5
x_ind = 1 : (n1 + n2)
thetahathat_b = array(NA, B)
for (b in 1 : B){
  x_1b = #TO-DO
  x_2b = #TO-DO
  thetahathat_b[b] = test_stat(x_1b, x_2b)
}
```

Now compute the test stat for the real data

```{r}
thetahathat = test_stat(x_1, x_2)
```

Declare the alpha value

```{r}
alpha = 0.05
```

Now run the two-sided test for difference in DGP at level alpha

```{r}
#TO-DO
```

What is the conclusion of the test?

#TO-DO

Calculate the p-value of this test.

```{r}
#TO-DO 
```

## Problem 3: The Bootstrap

Set the number of bootstrap samples to be `B = 100,000`.

```{r}
B = 1e5
```

Use the bootstrap to create a 95% CI for `theta := Med[X_2]` where X_2 is defined as the DGP of the population of weight increases for the CBT group. 

```{r}
thetahathat_b = array(NA, B)
for (b in 1 : B){
  thetahathat_b[b] = #TO-DO
}
#TO-DO
```

Use the bootstrap to test H_a: theta is nonzero where theta is defined as above.

```{r}
#TO-DO
```

Use the bootstrap to create a 95% CI for `theta := Q[X_2, 0.2]` i.e. the 20th percentile where X_2 is defined as before.

```{r}
thetahathat_b = array(NA, B)
for (b in 1 : B){
  thetahathat_b[b] = #TO-DO
}
#TO-DO
```

Use the bootstrap to test H_a: theta is nonzero where theta is defined as above.

```{r}
#TO-DO
```

## Problem 4: Parametric Survival using the Weibull iid DGP

Let's look at the lung dataset's survival by sex. We'll recode their censoring variable to match the definition from class. Group 1 is male and group 2 is female. The values are sorted.

```{r}
rm(list = ls())
is_male = 1 - (survival::lung$sex - 1) #zero is female, one is male
y_1 = survival::lung$time[is_male == 1]
c_1 = 1 - (survival::lung$status[is_male == 1] - 1)
y_2 = survival::lung$time[is_male == 0]
c_2 = 1 - (survival::lung$status[is_male == 0] - 1)
rm(is_male)
```

Using what you derived one the homework, for an iid Weibull DGP with no censoring, write a function that takes in the vector of survival times y and returns the values of the maximum likeliheood estimates of `k` and `lambda`. You'll need to call the `optimx` function within.

```{r}
mle_weibull_iid_compute = function(y){
  k_mle = #TO-DO 
  lambda_mle = #TO-DO
  list(k = k_mle, lambda = lambda_mle)
}
```

Pretend there is no censoring and find the maximum likelihood estimates of `k` and `lambda` for female survival times (group 2 only). 

```{r}
mles = mle_weibull_iid_compute(y_2)
mles$k
mles$lambda
```

Using the maximum likelihood estimates of `k` and `lambda`, compute the maximum likelihood estimate of the mean for female survival times (group 2 only).

```{r}
1 / mles$lambda * gamma(1 + 1 / mles$k)
```


Using what you derived one the homework for the setting where there is censoring at all different times given by the indices where `c_2 = 1`, find the MLE's of `k` and `lambda` for female survival times (group 2 only). You'll need the `optimx` function.

```{r}
#TO-DO
```


## Problem 5: Nonparametric Survival

For this problem, you'll need to read up on how to use the `survival` package as the solutions will all be one liners.

Trace out the Kaplan-Meier survival distribution estimate for female survival times (group 2 only).

```{r}
#TO-DO
```

Estimate median survival for females.

```{r}
#TO-DO
```

Run the log rank test to attempt to prove male and female survival are different.

```{r}
#TO-DO
```

Masters students only: run a bootstrap test at level 5% attempting to prove male and female median survival are different.

```{r}
#TO-DO
```

